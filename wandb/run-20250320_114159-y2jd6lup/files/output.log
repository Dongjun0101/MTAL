{'weight_decay': 0, 'run_no': 1, 'save_dir': 'save_temp', 'save_every': 5, 'biased': False, 'level': 32, 'compress': False, 'device': 0, 'workers': 4, 'port': '29500', 'method': 'svd', 'skew': 0.0, 'n_clients': 1, 'frac': 1, 'momentum': 0.0, 'increment_th': 0.001, 'lr': 0.01, 'batch_size': 64, 'deterministic': True, 'arch': 'alexnet', 'dataset': 'cifar100', 'classes': 100, 'num_tasks': 10, 'print_times': 5, 'wandb': True, 'gpmflag': True, 'seed': 1234, 'local_epochs': 30, 'buffer_size': 1000, 'alpha': 1.0}
Number of GPU available:  1
Files already downloaded and verified
Files already downloaded and verified
Data partition_sizes among clients: [1.0]
rank 0's total num of datasample in task0:  5056
Epoch: 0, Task: 0, Avg train Loss: 2.0870
Epoch: 1, Task: 0, Avg train Loss: 1.7792
Epoch: 2, Task: 0, Avg train Loss: 1.6381
Epoch: 3, Task: 0, Avg train Loss: 1.5590
Epoch: 4, Task: 0, Avg train Loss: 1.4716
Epoch: 5, Task: 0, Avg train Loss: 1.4316, Current Val Acc: 60.60, Accumulated Val Acc: 60.60
Epoch: 6, Task: 0, Avg train Loss: 1.3896
Epoch: 7, Task: 0, Avg train Loss: 1.3054
Epoch: 8, Task: 0, Avg train Loss: 1.2847
Epoch: 9, Task: 0, Avg train Loss: 1.2785
Epoch: 10, Task: 0, Avg train Loss: 1.2541
Epoch: 11, Task: 0, Avg train Loss: 1.2189, Current Val Acc: 65.60, Accumulated Val Acc: 65.60
Epoch: 12, Task: 0, Avg train Loss: 1.1874
Epoch: 13, Task: 0, Avg train Loss: 1.1774
Epoch: 14, Task: 0, Avg train Loss: 1.1788
Epoch: 15, Task: 0, Avg train Loss: 1.1343
Epoch: 16, Task: 0, Avg train Loss: 1.0674
Epoch: 17, Task: 0, Avg train Loss: 1.0620, Current Val Acc: 68.50, Accumulated Val Acc: 68.50
Epoch: 18, Task: 0, Avg train Loss: 1.0878
Epoch: 19, Task: 0, Avg train Loss: 1.0628
Epoch: 20, Task: 0, Avg train Loss: 1.0299
Epoch: 21, Task: 0, Avg train Loss: 1.0409
Epoch: 22, Task: 0, Avg train Loss: 1.0364
Epoch: 23, Task: 0, Avg train Loss: 1.0257, Current Val Acc: 71.70, Accumulated Val Acc: 71.70
Epoch: 24, Task: 0, Avg train Loss: 1.0464
Epoch: 25, Task: 0, Avg train Loss: 1.0406
Epoch: 26, Task: 0, Avg train Loss: 1.0515
Epoch: 27, Task: 0, Avg train Loss: 1.0348
Epoch: 28, Task: 0, Avg train Loss: 1.0334
Epoch: 29, Task: 0, Avg train Loss: 1.0259, Current Val Acc: 71.80, Accumulated Val Acc: 71.80
Node 0 Overall Accuracies:
	 70.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%
Traceback (most recent call last):
  File "fdr_single.py", line 964, in <module>
    w, avg_loss = client.train_epoch()  # train on single epoch
  File "fdr_single.py", line 814, in train_epoch
    loss_total.backward()
  File "/home/dk32578/anaconda3/envs/DJ/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/home/dk32578/anaconda3/envs/DJ/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.
Traceback (most recent call last):
  File "fdr_single.py", line 964, in <module>
    w, avg_loss = client.train_epoch()  # train on single epoch
  File "fdr_single.py", line 814, in train_epoch
    loss_total.backward()
  File "/home/dk32578/anaconda3/envs/DJ/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/home/dk32578/anaconda3/envs/DJ/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.
