{'weight_decay': 0, 'run_no': 1, 'save_dir': 'save_temp', 'save_every': 5, 'biased': False, 'level': 32, 'compress': False, 'device': 0, 'workers': 4, 'port': '29500', 'method': 'svd', 'skew': 0.0, 'n_clients': 1, 'frac': 1, 'momentum': 0.0, 'increment_th': 0.001, 'lr': 0.01, 'batch_size': 64, 'deterministic': True, 'arch': 'alexnet', 'dataset': 'cifar100', 'classes': 100, 'num_tasks': 10, 'print_times': 5, 'wandb': True, 'baseline': 'gpm', 'seed': 1, 'local_epochs': 50, 'buffer_size': 617, 'threshold': 0.4, 'alpha': 0.1, 'beta': 0.5, 'alpha_fdr': 0.6}
Random seed set to: 1
Number of GPU available:  1
Files already downloaded and verified
Files already downloaded and verified
Data partition_sizes among clients: [1.0]
----------------------------------------
Epoch: 0, Task: 0, Avg train Loss: 2.0814
Epoch: 1, Task: 0, Avg train Loss: 1.7815
Epoch: 2, Task: 0, Avg train Loss: 1.6517
Epoch: 3, Task: 0, Avg train Loss: 1.5388
Epoch: 4, Task: 0, Avg train Loss: 1.4743
Epoch: 5, Task: 0, Avg train Loss: 1.4251
Epoch: 6, Task: 0, Avg train Loss: 1.3484
Epoch: 7, Task: 0, Avg train Loss: 1.3117
Epoch: 8, Task: 0, Avg train Loss: 1.2622
Epoch: 9, Task: 0, Avg train Loss: 1.2486, Current Val Acc: 57.80, Accumulated Val Acc: 57.80
Epoch: 10, Task: 0, Avg train Loss: 1.2132
Epoch: 11, Task: 0, Avg train Loss: 1.1939
Epoch: 12, Task: 0, Avg train Loss: 1.1692
Epoch: 13, Task: 0, Avg train Loss: 1.1507
Epoch: 14, Task: 0, Avg train Loss: 1.1223
Epoch: 15, Task: 0, Avg train Loss: 1.1227
Epoch: 16, Task: 0, Avg train Loss: 1.0902
Epoch: 17, Task: 0, Avg train Loss: 1.0884
Epoch: 18, Task: 0, Avg train Loss: 1.0783
Epoch: 19, Task: 0, Avg train Loss: 1.0631, Current Val Acc: 63.60, Accumulated Val Acc: 63.60
Epoch: 20, Task: 0, Avg train Loss: 1.0448
Epoch: 21, Task: 0, Avg train Loss: 1.0264
Epoch: 22, Task: 0, Avg train Loss: 1.0213
Epoch: 23, Task: 0, Avg train Loss: 1.0072
Epoch: 24, Task: 0, Avg train Loss: 0.9902
Epoch: 25, Task: 0, Avg train Loss: 0.9575
Epoch: 26, Task: 0, Avg train Loss: 0.9270
Epoch: 27, Task: 0, Avg train Loss: 0.9102
Epoch: 28, Task: 0, Avg train Loss: 0.8974
Epoch: 29, Task: 0, Avg train Loss: 0.9137, Current Val Acc: 72.90, Accumulated Val Acc: 72.90
Epoch: 30, Task: 0, Avg train Loss: 0.8886
Epoch: 31, Task: 0, Avg train Loss: 0.8819
Epoch: 32, Task: 0, Avg train Loss: 0.8953
Epoch: 33, Task: 0, Avg train Loss: 0.8930
Epoch: 34, Task: 0, Avg train Loss: 0.8827
Epoch: 35, Task: 0, Avg train Loss: 0.8848
Epoch: 36, Task: 0, Avg train Loss: 0.8703
Epoch: 37, Task: 0, Avg train Loss: 0.8630
Epoch: 38, Task: 0, Avg train Loss: 0.8603
Epoch: 39, Task: 0, Avg train Loss: 0.8663, Current Val Acc: 73.90, Accumulated Val Acc: 73.90
Epoch: 40, Task: 0, Avg train Loss: 0.8760
Epoch: 41, Task: 0, Avg train Loss: 0.8427
Epoch: 42, Task: 0, Avg train Loss: 0.8715
Epoch: 43, Task: 0, Avg train Loss: 0.8661
Epoch: 44, Task: 0, Avg train Loss: 0.8565
Epoch: 45, Task: 0, Avg train Loss: 0.8674
Epoch: 46, Task: 0, Avg train Loss: 0.8638
Epoch: 47, Task: 0, Avg train Loss: 0.8611
Epoch: 48, Task: 0, Avg train Loss: 0.8741
Epoch: 49, Task: 0, Avg train Loss: 0.8671, Current Val Acc: 74.50, Accumulated Val Acc: 74.50
------------------------------
Representation Matrix
------------------------------
Layer 1 : (48, 48)
Layer 2 : (576, 576)
Layer 3 : (512, 512)
Layer 4 : (1024, 125)
Layer 5 : (2048, 125)
------------------------------
Threshold of GPM:  [0.4 0.4 0.4 0.4 0.4]
Number of Added Components:  [0, 1, 0, 0, 2]
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 0/48
Layer 2 : 1/576
Layer 3 : 0/512
Layer 4 : 0/1024
Layer 5 : 2/2048
----------------------------------------
full precision gpm memory overhead:  0.017822265625 MB
----------------------------------------
Node 0 Overall Accuracies:
	 74.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%
----------------------------------------
Epoch: 0, Task: 1, Avg train Loss: 1.8741
Epoch: 1, Task: 1, Avg train Loss: 1.5505
Epoch: 2, Task: 1, Avg train Loss: 1.4567
Epoch: 3, Task: 1, Avg train Loss: 1.3993
Epoch: 4, Task: 1, Avg train Loss: 1.3811
Epoch: 5, Task: 1, Avg train Loss: 1.3429
Epoch: 6, Task: 1, Avg train Loss: 1.3054
Epoch: 7, Task: 1, Avg train Loss: 1.2811
Epoch: 8, Task: 1, Avg train Loss: 1.2527
Epoch: 9, Task: 1, Avg train Loss: 1.2449, Current Val Acc: 57.60, Accumulated Val Acc: 61.35
Epoch: 10, Task: 1, Avg train Loss: 1.2393
Epoch: 11, Task: 1, Avg train Loss: 1.2089
Epoch: 12, Task: 1, Avg train Loss: 1.1901
Epoch: 13, Task: 1, Avg train Loss: 1.1971
Epoch: 14, Task: 1, Avg train Loss: 1.1436
Epoch: 15, Task: 1, Avg train Loss: 1.1362
Epoch: 16, Task: 1, Avg train Loss: 1.1376
Epoch: 17, Task: 1, Avg train Loss: 1.1165
Epoch: 18, Task: 1, Avg train Loss: 1.1173
Epoch: 19, Task: 1, Avg train Loss: 1.0941, Current Val Acc: 63.80, Accumulated Val Acc: 64.00
Traceback (most recent call last):
  File "main.py", line 149, in <module>
    main()
  File "main.py", line 141, in main
    train(args)
  File "/home/dk32578/project/QGPM_baseline/trainer.py", line 80, in train
    w, avg_loss = client.train_epoch()  # train on single epoch
  File "/home/dk32578/project/QGPM_baseline/methods/gpm.py", line 231, in train_epoch
    outputs = self.model(input_var)
  File "/home/dk32578/anaconda3/envs/DJ/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/dk32578/anaconda3/envs/DJ/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/dk32578/project/QGPM_baseline/models/alexnet_model.py", line 120, in forward
    x = self.drop2(self.relu(self.bn4(x)))
  File "/home/dk32578/anaconda3/envs/DJ/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/dk32578/anaconda3/envs/DJ/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/dk32578/anaconda3/envs/DJ/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/dk32578/anaconda3/envs/DJ/lib/python3.8/site-packages/torch/nn/functional.py", line 2462, in batch_norm
    if has_torch_function_variadic(input, running_mean, running_var, weight, bias):
KeyboardInterrupt
Traceback (most recent call last):
  File "main.py", line 149, in <module>
    main()
  File "main.py", line 141, in main
    train(args)
  File "/home/dk32578/project/QGPM_baseline/trainer.py", line 80, in train
    w, avg_loss = client.train_epoch()  # train on single epoch
  File "/home/dk32578/project/QGPM_baseline/methods/gpm.py", line 231, in train_epoch
    outputs = self.model(input_var)
  File "/home/dk32578/anaconda3/envs/DJ/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/dk32578/anaconda3/envs/DJ/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/dk32578/project/QGPM_baseline/models/alexnet_model.py", line 120, in forward
    x = self.drop2(self.relu(self.bn4(x)))
  File "/home/dk32578/anaconda3/envs/DJ/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/dk32578/anaconda3/envs/DJ/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/dk32578/anaconda3/envs/DJ/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/dk32578/anaconda3/envs/DJ/lib/python3.8/site-packages/torch/nn/functional.py", line 2462, in batch_norm
    if has_torch_function_variadic(input, running_mean, running_var, weight, bias):
KeyboardInterrupt
